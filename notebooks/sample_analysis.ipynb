{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c9ca7c",
   "metadata": {},
   "source": [
    "In this notebook, we will showcase some simple analysis on the output of the physics-based tropical cyclone downscaling mode. First, we will import some standard packages, which should aid in our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6538da6f",
   "metadata": {},
   "source": [
    "Next, we will assume you have ran the model with some output. Here, we will offer five sample ensemble member files, which which represent a tropical cyclone downscaling of the North Atlantic, from 1979-2023, with 14 tracks per year. We have five 'ensemble member' files, generated by running the model twice. In general, a much larger number of tracks is recommended to do robust statistical analysis on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9006da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_list = ['data/tracks_NA_era5_197901_202312.nc',\n",
    "           'data/tracks_NA_era5_197901_202312_e0.nc',\n",
    "           'data/tracks_NA_era5_197901_202312_e1.nc',\n",
    "           'data/tracks_NA_era5_197901_202312_e2.nc',\n",
    "           'data/tracks_NA_era5_197901_202312_e3.nc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623016a",
   "metadata": {},
   "source": [
    "Next, we will open the files, using `xarray`, by concatenating along a new dimension, 'ensemble'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open each of the files, by concatenating along a new dimension, 'ensemble'.\n",
    "ds = xr.open_mfdataset(fn_list, concat_dim='ensemble', combine='nested')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c8ef6",
   "metadata": {},
   "source": [
    "Now, we will have to set some parameters in order to calculate a few quantities, such as the seasonal cycle and interannual frequency. Here, we set the `tracks_per_year` variable, which was the values used in the namelist file when generating the sample data. `tracks_per_year` is the number of tracks to simulate per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cfbff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all the variables here in the namelist file are\n",
    "# consistent with ther values used to generate your data!\n",
    "tracks_per_year = 14 # equal to namelist.tracks_per_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0170f5b8",
   "metadata": {},
   "source": [
    "First, we will calculate the seasonal cycle. In order to calculate the seasonal cycle for a particular basin, we need to use the 'tc_basins' variable to select for that basin, and use that mask to access the month of the track. The below code does this and plots the seasonal cycle. Note, we only downscaled the North Atlantic here, so you will have to run the model yourself to look at the modeled seasonal cycle for other basins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_id = 'NA' # see ds['basin'] for a list of basin IDs\n",
    "mask = ds['tc_basins'] == basin_id\n",
    "plt.hist(ds['tc_month'].where(mask).load().data.flatten(),\n",
    "         density = True, bins = np.arange(-0.5, 12.6, 1), width = 0.90)\n",
    "plt.xlabel('Month'); plt.ylabel('Density')\n",
    "plt.xlim([0.5, 12.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff05944c",
   "metadata": {},
   "source": [
    "Next, we will calculate the interannual frequency predicted by the model. This is a little trickier, since we need to understand the random seeding process developed by Kerry Emanuel. The model randomly seeds storms every where, and evolves them in space and time. Most of the seeds die out, but a few survive, which are kept as the storms. Let's say that $N_{seeds}$ number of seeds were needed to generate $N_{storms}$ number of storms. Then the 'seed survival rate', $\\Gamma$ can be defined as:\n",
    "$$\n",
    "    \\Gamma = \\frac{N_{\\text{storms}}}{N_{\\text{seeds}}}\n",
    "$$\n",
    "which has units storms per seed. Note, in order to convert this into a number of storms (i.e. a frequency), we need to multiply by a singular constant with units seeds. Here is where we assume a *fixed* seeding frequency across all years (see Emanuel 2022 for more nuanced discussion), $c$, with units seeds per year. Then, the frequency of storms per year, $f_{\\text{storms}}$ is simply:\n",
    "$$\n",
    "    f_{\\text{storms}} = c \\Gamma\n",
    "$$\n",
    "\n",
    "In this model, $N_{\\text{storms}}$ is set by the user (`tracks_per_year`), and $N_{\\text{seeds}}$ is given by the model. However, $c$ is unknown, and can be thought of as a 'tuning' parameter. Here, we will determine $c$ by simply setting it so that the long term average of $f_{\\text{storms}}$ is equal to the observed long-term average. However, we should be clear that the inherent assumptions here are a subject of open research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad90b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of seeds, per month, needed to generate a fixed number\n",
    "# of storms per year, summed across the 'ensemble members' (file).\n",
    "total_seeds = ds['seeds_per_month'].sum('ensemble')\n",
    "total_seeds_per_year = ds['seeds_per_month'].sum(['ensemble', 'month'])\n",
    "total_seeds_per_year_NA = total_seeds_per_year.sel(basin = 'NA')\n",
    "\n",
    "# Calculate interannual frequency, setting the average frequency\n",
    "# equal to 'obs_tracks_per_year', by multiplying by a constant.\n",
    "obs_tracks_per_year = 14    # should be calculated using observations\n",
    "gamma = tracks_per_year / total_seeds_per_year_NA\n",
    "c = obs_tracks_per_year / gamma.mean()  # seeding frequency constant\n",
    "storm_freq = c * gamma\n",
    "\n",
    "plt.plot(ds['year'], storm_freq)\n",
    "plt.xlim([ds['year'][0], ds['year'][-1]])\n",
    "plt.xlabel('Year'); plt.ylabel('Number of Storms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcad42e",
   "metadata": {},
   "source": [
    "Finally, in this section, we will include some sample analysis on retrieving return period analyses for singular points in space. It is important to remember here that generally, a large number of synthetic tracks are necessary to robustly estimate return period. However, for the purposes of this notebook, we will use just the five provided sample files, so the estimates will *not* be robust. We will use the procedure defined in Lin et al. (2025), which looks at the maximum intensity of all storms that pass within 100-km of a point, which we will choose to be Miami (feel free to set this to whatever point you'd like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c1fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haversine function, which calculates the great-circle\n",
    "# distances between two points [lon1, lat1], [lon2, lat2].\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    # Convert decimal degrees to radians\n",
    "    lon1 = np.deg2rad(lon1)\n",
    "    lat1 = np.deg2rad(lat1)\n",
    "    lon2 = np.deg2rad(lon2)\n",
    "    lat2 = np.deg2rad(lat2)\n",
    "\n",
    "    # Use the Haversine formula.\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = (np.square(np.sin(dlat/2)) + np.cos(lat1) *\n",
    "         np.cos(lat2) * np.square(np.sin(dlon/2)))\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "    r_earth = 6378000   # m\n",
    "    km = (r_earth / 1000.) * c\n",
    "    return(km)\n",
    "\n",
    "# Point of interest longitude/latitude coordinates, Miami.\n",
    "clat = 25.7617\n",
    "clon = -80.1918"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ce0ad2",
   "metadata": {},
   "source": [
    "Now, we will use the powerful abilities of `xarray` to find the maximum intensity of each storm, when it is within 100-km of the selected point of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance of each track from the selected point of interest.\n",
    "dists = haversine(clon, clat, ds['lon_trks'], ds['lat_trks']).load()\n",
    "\n",
    "# Find the maximum intensity of each storm, when it is within\n",
    "# 100-km of the selected point of interest.\n",
    "vmax_at_poi = ds['vmax_trks'].where(dists <= 100).max(dim = 'time').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38a49e",
   "metadata": {},
   "source": [
    "Finally, to calculate the return period, notice that we have 2 ensemble members, each simulating 45 years of tropical cyclone activity. This is a total of 90 years, so we can estimate the intensity of up to the 1 in 90 year storm. As noted earlier, this is going to be subject to a lot of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf20079",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_years = len(ds['year']) * len(ds['ensemble'])\n",
    "\n",
    "# Calculate the number of instances where a storm exceeded\n",
    "vmax_bins = np.arange(10, 81, 5)\n",
    "exceedance_count = np.zeros(len(vmax_bins))\n",
    "vmax_at_poi_flat = vmax_at_poi.data.flatten()\n",
    "for i in range(len(vmax_bins)):\n",
    "    exceedance_count[i] = np.sum(vmax_at_poi_flat >= vmax_bins[i])\n",
    "\n",
    "# Plot, on a log y-scale, the return period of the intensity\n",
    "plt.plot(vmax_bins*1.94384, np.log(total_years / exceedance_count))\n",
    "plt.yticks(np.log(np.array([2, 5, 10, 20, 30, 50, 100])),\n",
    "           [2, 5, 10, 20, 30, 50, 100])\n",
    "plt.xlabel('Intensity (knots))')\n",
    "plt.ylabel('Return Period (years)')\n",
    "plt.grid()\n",
    "plt.ylim([np.log(2), np.log(100)])\n",
    "plt.xlim([50, 150])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tc_risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
